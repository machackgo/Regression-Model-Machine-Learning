{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Lp16gstLKT6DfhTPNsbwdG3BgEjWDZIO","timestamp":1719113502475}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CazISR8X_HUG"},"source":["# Multiple Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"pOyqYHTk_Q57"},"source":["## Importing the libraries"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"metadata":{"id":"HLEYq5GQJzqU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vgC61-ah_WIz"},"source":["## Importing the dataset"]},{"cell_type":"code","source":["dataset = pd.read_csv('50_Startups.csv')\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values"],"metadata":{"id":"6huFFijfKavd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset)"],"metadata":{"id":"Veezy_k7NMdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X)"],"metadata":{"id":"D8dAjyMkNQKg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VadrvE7s_lS9"},"source":["## Encoding categorical data"]},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[3])], remainder='passthrough')\n","\n","X = np.array(ct.fit_transform(X))\n"],"metadata":{"id":"nxrPjmdcL72y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_WmT2czNiJ-","executionInfo":{"status":"ok","timestamp":1719114544336,"user_tz":-330,"elapsed":4,"user":{"displayName":"Mohammed Faraz","userId":"09790691887558301050"}},"outputId":"880fc4e1-552d-4d97-ab47-1c1efe85a05d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n"," [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n"," [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n"," [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n"," [0.0 1.0 0.0 142107.34 91391.77 366168.42]\n"," [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n"," [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n"," [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n"," [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n"," [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n"," [0.0 1.0 0.0 101913.08 110594.11 229160.95]\n"," [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n"," [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n"," [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n"," [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n"," [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n"," [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n"," [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n"," [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n"," [0.0 0.0 1.0 86419.7 153514.11 0.0]\n"," [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n"," [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n"," [0.0 1.0 0.0 73994.56 122782.75 303319.26]\n"," [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n"," [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n"," [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n"," [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n"," [0.0 0.0 1.0 72107.6 127864.55 353183.81]\n"," [0.0 1.0 0.0 66051.52 182645.56 118148.2]\n"," [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n"," [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n"," [0.0 0.0 1.0 61136.38 152701.92 88218.23]\n"," [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n"," [0.0 1.0 0.0 55493.95 103057.49 214634.81]\n"," [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n"," [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n"," [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n"," [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n"," [0.0 0.0 1.0 20229.59 65947.93 185265.1]\n"," [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n"," [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n"," [0.0 1.0 0.0 27892.92 84710.77 164470.71]\n"," [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n"," [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n"," [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n"," [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n"," [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n"," [1.0 0.0 0.0 0.0 135426.92 0.0]\n"," [0.0 0.0 1.0 542.05 51743.15 0.0]\n"," [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"WemVnqgeA70k"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"],"metadata":{"id":"m3_uWR6cKcSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test)\n","print(y_test)"],"metadata":{"id":"cnwoFs9cuW3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-McZVsQBINc"},"source":["## Training the Multiple Linear Regression model on the Training set"]},{"cell_type":"code","source":["# the class which we are going to call will take cake of the dummy variable trap and\n","# the backwards elimination process which we have studied is previously [pg-49] this class\n","# consistes of inbuild backwards elimination process such that we dont have to do it suprately\n","# and we dont have to wory about which varibale consistes of high or low p-value\n","# and we are using the same class that we have used for simple linear regression model"],"metadata":{"id":"8zL01P4QPNVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)"],"metadata":{"id":"ENCzhDVXi3te"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNkXL1YQBiBT"},"source":["## Predicting the Test set results"]},{"cell_type":"code","source":["# so in this cell we have combared real test values with predicted test values\n","# but if you want you can compare real train values with predicted train values\n","# the point is there dimension should be same like real train values with predicted train values\n","# and real test values with predicted test values\n","\n","# precision = 2 means it will display only 2 values after comma like  before 22.344444 after 22.34\n","# concatenate is used to combine two vectors or combining elements for that we are using concatenate() class\n","# now if we print y_pred it will give us output in an vertical row but if we want our output in an horizontal column\n","# converting that 1 row to column then we have to use reshape under that how must is the length of the vector and\n","# and vector name\n","# axis =0 means do you want a vertical concatenation and axis =1 means do yo want a horizontal concatination\n","y_pred = regressor.predict(X_test)\n","# print(y_pred)\n","np.set_printoptions(precision=2)\n","#                        vector of predicted profit|  vector of real profit\n","print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n","                                              # |how many column do you want       |axis = 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd5jAUNemLDB","executionInfo":{"status":"ok","timestamp":1719174209455,"user_tz":-330,"elapsed":573,"user":{"displayName":"Mohammed Faraz","userId":"09790691887558301050"}},"outputId":"b49db5e6-dacb-43b0-ffec-70deda7dd21c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[103015.2  103282.38]\n"," [132582.28 144259.4 ]\n"," [132447.74 146121.95]\n"," [ 71976.1   77798.83]\n"," [178537.48 191050.39]\n"," [116161.24 105008.31]\n"," [ 67851.69  81229.06]\n"," [ 98791.73  97483.56]\n"," [113969.44 110352.25]\n"," [167921.07 166187.94]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xlbIdHcRwxFZ"},"execution_count":null,"outputs":[]}]}